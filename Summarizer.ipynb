{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import operator\n",
    "\n",
    "from data_loader import DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk import pos_tag\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import pke\n",
    "\n",
    "from  itertools import chain\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lijia\\Desktop\\MSAI\\Sem A\\AI6122\\Assignment1\\Hearts-of-Iron\\data_loader.py:14: FutureWarning: Starting with pandas version 2.0 all arguments of read_json except for the argument 'path_or_buf' will be keyword-only.\n",
      "  self.table = pd.read_json(data_path, 'records', lines = True);\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>A1C0C9IJO2HS03</td>\n",
       "      <td>B00000016W</td>\n",
       "      <td>A Customer</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Brian Wilson was so blown away after hearing R...</td>\n",
       "      <td>5</td>\n",
       "      <td>Holy jeez this is good!</td>\n",
       "      <td>1044489600</td>\n",
       "      <td>02 6, 2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>A22N9H8V0RYQR3</td>\n",
       "      <td>B00000016W</td>\n",
       "      <td>A fair and Balanced Rater</td>\n",
       "      <td>[4, 29]</td>\n",
       "      <td>I never understood what's the BIG deal behind ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Overrated HYPE</td>\n",
       "      <td>1125273600</td>\n",
       "      <td>08 29, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>A1E110L9ZKX6FD</td>\n",
       "      <td>B00000016W</td>\n",
       "      <td>All Powerful Wizard Of Oz</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>For those that say they &amp;quot;don't get it&amp;quo...</td>\n",
       "      <td>5</td>\n",
       "      <td>Lets Break it down, coming from a musican.....</td>\n",
       "      <td>1078185600</td>\n",
       "      <td>03 2, 2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>A3HU0B9XUEVHIM</td>\n",
       "      <td>B00000016W</td>\n",
       "      <td>Andre S. Grindle \"Andre' Grindle\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Brian Wilson once said of this album he wanted...</td>\n",
       "      <td>5</td>\n",
       "      <td>It Was Uniquely Wonderful Than. And It Still R...</td>\n",
       "      <td>1321056000</td>\n",
       "      <td>11 12, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>A252HNVAZENVNE</td>\n",
       "      <td>B00000016W</td>\n",
       "      <td>Andre' S Grindle \"Funk Meister\"</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>You know,even with classic radio hits like &amp;qu...</td>\n",
       "      <td>5</td>\n",
       "      <td>A Classic  All The Way Through!</td>\n",
       "      <td>1060732800</td>\n",
       "      <td>08 13, 2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID        asin                       reviewerName  helpful  \\\n",
       "85  A1C0C9IJO2HS03  B00000016W                         A Customer   [1, 1]   \n",
       "86  A22N9H8V0RYQR3  B00000016W          A fair and Balanced Rater  [4, 29]   \n",
       "87  A1E110L9ZKX6FD  B00000016W          All Powerful Wizard Of Oz  [9, 10]   \n",
       "88  A3HU0B9XUEVHIM  B00000016W  Andre S. Grindle \"Andre' Grindle\"   [0, 0]   \n",
       "89  A252HNVAZENVNE  B00000016W    Andre' S Grindle \"Funk Meister\"   [1, 3]   \n",
       "\n",
       "                                           reviewText  overall  \\\n",
       "85  Brian Wilson was so blown away after hearing R...        5   \n",
       "86  I never understood what's the BIG deal behind ...        1   \n",
       "87  For those that say they &quot;don't get it&quo...        5   \n",
       "88  Brian Wilson once said of this album he wanted...        5   \n",
       "89  You know,even with classic radio hits like &qu...        5   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "85                            Holy jeez this is good!      1044489600   \n",
       "86                                     Overrated HYPE      1125273600   \n",
       "87     Lets Break it down, coming from a musican.....      1078185600   \n",
       "88  It Was Uniquely Wonderful Than. And It Still R...      1321056000   \n",
       "89                    A Classic  All The Way Through!      1060732800   \n",
       "\n",
       "     reviewTime  \n",
       "85   02 6, 2003  \n",
       "86  08 29, 2005  \n",
       "87   03 2, 2004  \n",
       "88  11 12, 2011  \n",
       "89  08 13, 2003  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(DataLoader.data_path2)\n",
    "table = loader.load_table()\n",
    "# table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk:  180\n",
      "spacy:  326\n",
      "merge:  383\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocess\n",
    "#Get stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords.append('\\n')\n",
    "spacy_stopwords = list(STOP_WORDS)\n",
    "merged_sw = nltk_stopwords + list(set(spacy_stopwords) - set(nltk_stopwords))\n",
    "\n",
    "# print(merged_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the review text dictionary\n",
    "reviewText = {}\n",
    "for i in table.index:\n",
    "    if table['asin'][i] in reviewText:\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)\n",
    "    else:\n",
    "        reviewText[table['asin'][i]] = []\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(original_review, stop_words):\n",
    "    # to lowercase\n",
    "    review = original_review.lower()\n",
    "    # remove punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', ' ', review).strip()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_words = review.split(' ')\n",
    "    review_words_tag = nltk.pos_tag(review_words)\n",
    "    processed_review = \"\"\n",
    "    # remove stop words&lemma\n",
    "    for word, tag in review_words_tag:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma_word = word\n",
    "        else:\n",
    "            lemma_word = lemmatizer.lemmatize(word, wntag)\n",
    "        if lemma_word not in stop_words:\n",
    "            processed_review = processed_review + lemma_word + \" \"\n",
    "    processed_review = \" \".join(processed_review.split())\n",
    "    processed_review = processed_review.strip()\n",
    "    return processed_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviewText = {}\n",
    "for r in reviewText:\n",
    "    processed_reviewText[r] = []\n",
    "    for t in reviewText[r]:\n",
    "        p = data_preprocess(t, merged_sw)\n",
    "        processed_reviewText[r].append(p)\n",
    "# print(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect frequency\n",
    "\n",
    "def collect_frequency(processed_review):\n",
    "    word_frequency = defaultdict(lambda:0)\n",
    "    for v in processed_review.values():\n",
    "        for i in v:\n",
    "            tokens = nltk.word_tokenize(i)\n",
    "            for token in tokens:\n",
    "                word_frequency[token] += 1  \n",
    "    return word_frequency\n",
    "\n",
    "frequency = collect_frequency(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = sorted(frequency.items(), key=lambda item: item[1],reverse=True)\n",
    "# print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_processed_reviewText = sorted(processed_reviewText.items(), key=lambda item: len(item[1]),reverse=True)\n",
    "# print(sorted_processed_reviewText[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3027\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_reviewText[sorted_processed_reviewText[0][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\"\"\n",
    "for i in reviewText[sorted_processed_reviewText[0][0]]:\n",
    "    text+=i\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models Cite from https://github.com/boudinfl/pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a good rap album .', 0.019971753091184406), ('a good rap album', 0.01892312442541186), ('other commercial rap album', 0.017162462137877112), ('a real gangsta rap album', 0.016830597486591596), ('many good mainstream rap albums', 0.01578108772295227), ('a good album .', 0.015650519909826452), ('the previous album the last album', 0.015634677769476645), ('a good song 5/55.piggy bank', 0.014602359354647088), ('album iz good', 0.014434457187851154), ('a good album', 0.014172151244053906)]\n",
      "4.067541122436523\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#TextRank\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('baltimore love thing', 7.136342516303575e-05), ('candy shop', 0.00015989046436451352), ('die tryin', 0.0002617178314869215), ('ski mask way', 0.00026577914464383776), ('die tonight', 0.0002671537525698024), ('baltimore love', 0.00028511253329892213), ('album', 0.00030649095792677894), ('cent', 0.00032256545009830615), ('love thing', 0.0003644474800517442), ('die', 0.00047382669454123176)]\n",
      "7.242422580718994\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None,\n",
    "                        stoplist=merged_sw)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8\n",
    "keyphrases = extractor.get_n_best(n=10, threshold=threshold)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('album', 887.5790004038475), ('like', 664.0992878021644), ('song', 505.60303773004887), ('cent', 340.7669376550486), ('songs', 313.82257514278893), ('love', 275.7834751254812), ('beat', 269.4436251225966), ('die', 263.10377511971194), ('50 cent', 253.594000115385), ('music', 239.3293376088946)]\n",
      "6.076050758361816\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# 1. create a TfIdf extractor.\n",
    "extractor = pke.unsupervised.TfIdf()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += pke.lang.stopwords.get('en')\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        stoplist=merged_sw,\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using a `tf` x `idf`\n",
    "df = pke.load_document_frequency_file(input_file='df.tsv.gz')\n",
    "extractor.candidate_weighting(df=df)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('album', 0.033753948362687455), ('song', 0.023967132116933014), ('beat', 0.014620312722206206), ('good', 0.013592454845410724), ('cent', 0.013138238057820358), ('track', 0.0109398281042443), ('game', 0.01057093102053484), ('good music', 0.009073734279705733), ('micsthe massacre', 0.008359861980728623), ('rich', 0.007891583946772711)]\n",
      "68.7447738647461\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# TopicRank\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load text\n",
    "extractor.load_document(input=text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAKE refers to https://github.com/fabianvf/python-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAKE Algorithm\n",
    "def is_num(current_word):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def split_words(text):\n",
    "    splitter = re.compile(r'(?u)\\W+')\n",
    "    \n",
    "    words = []\n",
    "    for singleWord in splitter.split(text):\n",
    "        currentWord = singleWord.strip().lower()\n",
    "        \n",
    "        if currentWord != '' and not is_num(currentWord):\n",
    "            words.append(currentWord)\n",
    "            \n",
    "    return words\n",
    "\n",
    "\n",
    "def build_regex(stopwords):\n",
    "    sw_regex_list = []\n",
    "    for word in stopwords:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'\n",
    "        sw_regex_list.append(word_regex)\n",
    "        \n",
    "    return re.compile('(?u)' + '|'.join(sw_regex_list), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def generate_keywords(sentences, stopWordPattern, minCharacters, maxWords):\n",
    "    phrases = []\n",
    "    for s in sentences:\n",
    "        tmp = re.sub(stopWordPattern, '|', s.strip())\n",
    "        ps = tmp.split(\"|\")\n",
    "        \n",
    "        for phrase in ps:\n",
    "            phrase = phrase.strip().lower()\n",
    "            \n",
    "            if phrase != '' and len(phrase) >= minCharacters and len(phrase.split()) <= maxWords:\n",
    "                phrases.append(phrase)\n",
    "                \n",
    "    return phrases\n",
    "\n",
    "\n",
    "def word_scores(phraseList):\n",
    "    frequency = {}\n",
    "    degree = {}\n",
    "    for phrase in phraseList:\n",
    "        wordList = split_words(phrase)\n",
    "        wordListLen = len(wordList)\n",
    "        wordListDegree = wordListLen - 1\n",
    "        for word in wordList:\n",
    "            frequency.setdefault(word, 0)\n",
    "            frequency[word] += 1\n",
    "            degree.setdefault(word, 0)\n",
    "            degree[word] += wordListDegree\n",
    "    for i in frequency:\n",
    "        degree[i] = frequency[i] + degree[i]\n",
    "    score = {}\n",
    "    for i in frequency:\n",
    "        score.setdefault(i, 0)\n",
    "        score[i] = degree[i] / (frequency[i] * 1.0)\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phraseList, wordScore, minFrequency):\n",
    "    ckScore = {}\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    for p in phraseList:\n",
    "        counts[p] += 1\n",
    "\n",
    "    for phrase in phraseList:\n",
    "        if counts[phrase] >= minFrequency:\n",
    "            ckScore.setdefault(phrase, 0)\n",
    "            wordList = split_words(phrase)\n",
    "            candidateScore = 0\n",
    "            for word in wordList:\n",
    "                candidateScore += wordScore[word]\n",
    "            ckScore[phrase] = candidateScore\n",
    "    return ckScore\n",
    "\n",
    "def Rake(text, minCharNum, maxWordNum, minFrequency):\n",
    "        sentences = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s').split(text)\n",
    "\n",
    "        phrases = generate_keywords(sentences, build_regex(merged_sw), minCharNum, maxWordNum)\n",
    "\n",
    "        wordScores = word_scores(phrases)\n",
    "\n",
    "        ck = generate_candidate_keyword_scores(phrases, wordScores, minFrequency)\n",
    "\n",
    "        sortedKeywords = sorted(ck.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        return sortedKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ski mask way 5/5', 14.479115796419167), ('3 1/2 stars', 10.963016643230908), ('4 1/2 stars', 10.61302421554652), ('song rating 3/10', 10.610885993321785), ('song rating 8/10', 10.302368220015278), ('song rating 9/10', 10.267736185383244), ('song rating 7/10', 10.151574569221628), ('50 comes pretty nice', 9.25118433530981), ('got shot 9 times', 9.113217646550979), ('outta control remix ft', 9.106313131313131)]\n",
      "0.5256614685058594\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "keywords = Rake(text, 2, 4, 2)\n",
    "print(keywords[:10])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1219d95d5844f6af4e0cf2878260c40dbf8742300f233955478158a2d2740a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
