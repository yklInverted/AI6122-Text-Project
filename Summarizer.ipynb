{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_loader import DataLoader\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "\n",
    "import numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(DataLoader.data_path2)\n",
    "table = loader.load_table()\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preprocess\n",
    "#Get stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords.append('\\n')\n",
    "spacy_stopwords = list(STOP_WORDS)\n",
    "merged_sw = nltk_stopwords + list(set(spacy_stopwords) - set(nltk_stopwords))\n",
    "\n",
    "print(\"nltk: \", len(nltk_stopwords))\n",
    "print(\"spacy: \", len(spacy_stopwords))\n",
    "print(\"merge: \", len(merged_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the review text dictionary\n",
    "reviewText = {}\n",
    "for i in range(200):\n",
    "    if table['asin'][i] in reviewText:\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)\n",
    "    else:\n",
    "        reviewText[table['asin'][i]] = []\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)\n",
    "print(reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(original_review, stop_words):\n",
    "    # to lowercase\n",
    "    review = original_review.lower()\n",
    "    # remove punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', ' ', review).strip()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_words = review.split(' ')\n",
    "    review_words_tag = nltk.pos_tag(review_words)\n",
    "    processed_review = \"\"\n",
    "    # remove stop words&lemma\n",
    "    for word, tag in review_words_tag:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma_word = word\n",
    "        else:\n",
    "            lemma_word = lemmatizer.lemmatize(word, wntag)\n",
    "        if lemma_word not in stop_words:\n",
    "            processed_review = processed_review + lemma_word + \" \"\n",
    "    processed_review = \" \".join(processed_review.split())\n",
    "    processed_review = processed_review.strip()\n",
    "    return processed_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviewText = {}\n",
    "for r in reviewText:\n",
    "    processed_reviewText[r] = []\n",
    "    for t in reviewText[r]:\n",
    "        p = data_preprocess(t, merged_sw)\n",
    "        processed_reviewText[r].append(p)\n",
    "print(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect frequency\n",
    "\n",
    "def collect_frequency(processed_review):\n",
    "    word_frequency = defaultdict(lambda:0)\n",
    "    for v in processed_review.values():\n",
    "        for i in v:\n",
    "            tokens = nltk.word_tokenize(i)\n",
    "            for token in tokens:\n",
    "                word_frequency[token] += 1  \n",
    "    return word_frequency\n",
    "\n",
    "frequency = collect_frequency(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = sorted(frequency.items(), key=lambda item: item[1],reverse=True)\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_processed_reviewText = sorted(processed_reviewText.items(), key=lambda item: len(item[1]),reverse=True)\n",
    "print(sorted_processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B001ESSLIU 41> B004EI3ON4 34 > 9714721180 16\n",
    "print(len(processed_reviewText['B001ESSLIU']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentencess, stopwords=None):\n",
    "    similarity_matrix = numpy.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stopwords)\n",
    "    return similarity_matrix\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def generate_summary(sentences, top_n=5):\n",
    "    summarize_text = []\n",
    "\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, None)\n",
    "\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
    "    for i in range(top_n):\n",
    "          summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = processed_reviewText['B001ESSLIU']\n",
    "generate_summary(sentences, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Abstractive Summarizer by Model\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "text = \"\"\"\"\"\"\n",
    "for i in reviewText['9714721180']:\n",
    "    text+=i\n",
    "print(text)\n",
    "\n",
    "batch = tokenizer.prepare_seq2seq_batch(text, truncation=True, padding='longest',return_tensors='pt').to(torch_device)\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rake_nltk to extract keyphrases\n",
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake()\n",
    "\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "r.get_ranked_phrases()[0:10]\n",
    "\n",
    "#python-rake to extract keyphrases\n",
    "import RAKE\n",
    "rake_object = RAKE.Rake(merged_sw)\n",
    "def sort_tuple(tup):\n",
    "    tup.sort(key = lambda x:x[1])\n",
    "    return tup\n",
    "keywords = sort_tuple(rake_object.run(text))[-10:]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1219d95d5844f6af4e0cf2878260c40dbf8742300f233955478158a2d2740a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
