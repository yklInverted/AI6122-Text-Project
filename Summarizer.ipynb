{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import operator\n",
    "\n",
    "from data_loader import DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk import pos_tag\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import pke\n",
    "\n",
    "from  itertools import chain\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(DataLoader.data_path2)\n",
    "table = loader.load_table()\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocess\n",
    "#Get stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords.append('\\n')\n",
    "spacy_stopwords = list(STOP_WORDS)\n",
    "merged_sw = nltk_stopwords + list(set(spacy_stopwords) - set(nltk_stopwords))\n",
    "\n",
    "print(\"nltk: \", len(nltk_stopwords))\n",
    "print(\"spacy: \", len(spacy_stopwords))\n",
    "print(\"merge: \", len(merged_sw))\n",
    "# print(merged_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the review text dictionary\n",
    "reviewText = {}\n",
    "for i in table.index:\n",
    "    if table['asin'][i] in reviewText:\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)\n",
    "    else:\n",
    "        reviewText[table['asin'][i]] = []\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(original_review, stop_words):\n",
    "    # to lowercase\n",
    "    review = original_review.lower()\n",
    "    # remove punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', ' ', review).strip()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_words = review.split(' ')\n",
    "    review_words_tag = nltk.pos_tag(review_words)\n",
    "    processed_review = \"\"\n",
    "    # remove stop words&lemma\n",
    "    for word, tag in review_words_tag:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma_word = word\n",
    "        else:\n",
    "            lemma_word = lemmatizer.lemmatize(word, wntag)\n",
    "        if lemma_word not in stop_words:\n",
    "            processed_review = processed_review + lemma_word + \" \"\n",
    "    processed_review = \" \".join(processed_review.split())\n",
    "    processed_review = processed_review.strip()\n",
    "    return processed_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviewText = {}\n",
    "for r in reviewText:\n",
    "    processed_reviewText[r] = []\n",
    "    for t in reviewText[r]:\n",
    "        p = data_preprocess(t, merged_sw)\n",
    "        processed_reviewText[r].append(p)\n",
    "# print(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect frequency\n",
    "\n",
    "def collect_frequency(processed_review):\n",
    "    word_frequency = defaultdict(lambda:0)\n",
    "    for v in processed_review.values():\n",
    "        for i in v:\n",
    "            tokens = nltk.word_tokenize(i)\n",
    "            for token in tokens:\n",
    "                word_frequency[token] += 1  \n",
    "    return word_frequency\n",
    "\n",
    "frequency = collect_frequency(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = sorted(frequency.items(), key=lambda item: item[1],reverse=True)\n",
    "# print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_processed_reviewText = sorted(processed_reviewText.items(), key=lambda item: len(item[1]),reverse=True)\n",
    "print(sorted_processed_reviewText[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DM B000084T18 2774> B00006690F 2625 > B00005O54Q 1719\n",
    "print(len(processed_reviewText['B000084T18']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #KS B006GWO5WK 3495> B0078S9B6G 1771 > B005C5YZ86 775\n",
    "# print(len(processed_reviewText['B0078S9B6G']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"\"\"\n",
    "for i in reviewText['B000084T18']:\n",
    "    text+=i\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#TextRank\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "keyphrases\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None,\n",
    "                        stoplist=merged_sw)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8\n",
    "keyphrases = extractor.get_n_best(n=10, threshold=threshold)\n",
    "\n",
    "keyphrases\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# 1. create a TfIdf extractor.\n",
    "extractor = pke.unsupervised.TfIdf()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += pke.lang.stopwords.get('en')\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        stoplist=merged_sw,\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using a `tf` x `idf`\n",
    "df = pke.load_document_frequency_file(input_file='df.tsv.gz')\n",
    "extractor.candidate_weighting(df=df)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "keyphrases\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# TopicRank\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load text\n",
    "extractor.load_document(input=text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "keyphrases\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAKE Algorithm\n",
    "def is_num(current_word):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def split_words(text):\n",
    "    splitter = re.compile(r'(?u)\\W+')\n",
    "    \n",
    "    words = []\n",
    "    for singleWord in splitter.split(text):\n",
    "        currentWord = singleWord.strip().lower()\n",
    "        \n",
    "        if currentWord != '' and not is_num(currentWord):\n",
    "            words.append(currentWord)\n",
    "            \n",
    "    return words\n",
    "\n",
    "\n",
    "def build_regex(stopwords):\n",
    "    sw_regex_list = []\n",
    "    for word in stopwords:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'\n",
    "        sw_regex_list.append(word_regex)\n",
    "        \n",
    "    return re.compile('(?u)' + '|'.join(sw_regex_list), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def generate_keywords(sentences, stopWordPattern, minCharacters, maxWords):\n",
    "    phrases = []\n",
    "    for s in sentences:\n",
    "        tmp = re.sub(stopWordPattern, '|', s.strip())\n",
    "        ps = tmp.split(\"|\")\n",
    "        \n",
    "        for phrase in ps:\n",
    "            phrase = phrase.strip().lower()\n",
    "            \n",
    "            if phrase != '' and len(phrase) >= minCharacters and len(phrase.split()) <= maxWords:\n",
    "                phrases.append(phrase)\n",
    "                \n",
    "    return phrases\n",
    "\n",
    "\n",
    "def word_scores(phraseList):\n",
    "    frequency = {}\n",
    "    degree = {}\n",
    "    \n",
    "    for phrase in phraseList:\n",
    "        wordList = split_words(phrase)\n",
    "        wordListLen = len(wordList)\n",
    "        wordListDegree = wordListLen - 1\n",
    "        \n",
    "        for word in wordList:\n",
    "            frequency.setdefault(word, 0)\n",
    "            frequency[word] += 1\n",
    "            degree.setdefault(word, 0)\n",
    "            degree[word] += wordListDegree\n",
    "            \n",
    "    for i in frequency:\n",
    "        degree[i] = frequency[i] + degree[i]\n",
    "\n",
    "    score = {}\n",
    "    for i in frequency:\n",
    "        score.setdefault(i, 0)\n",
    "        score[i] = degree[i] / (frequency[i] * 1.0)\n",
    "        \n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phraseList, wordScore, minFrequency):\n",
    "    ckScore = {}\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    for p in phraseList:\n",
    "        counts[p] += 1\n",
    "\n",
    "    for phrase in phraseList:\n",
    "        if counts[phrase] >= minFrequency:\n",
    "            ckScore.setdefault(phrase, 0)\n",
    "            wordList = split_words(phrase)\n",
    "            candidateScore = 0\n",
    "            for word in wordList:\n",
    "                candidateScore += wordScore[word]\n",
    "            ckScore[phrase] = candidateScore\n",
    "    return ckScore\n",
    "\n",
    "def Rake(text, minCharNum, maxWordNum, minFrequency):\n",
    "#       split_sentences\n",
    "        sentences = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s').split(text)\n",
    "\n",
    "        phrases = generate_keywords(sentences, build_regex(merged_sw), minCharNum, maxWordNum)\n",
    "\n",
    "        wordScores = word_scores(phrases)\n",
    "\n",
    "        ck = generate_candidate_keyword_scores(phrases, wordScores, minFrequency)\n",
    "\n",
    "        sortedKeywords = sorted(ck.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        return sortedKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "keywords = Rake(text, 2, 4, 2)\n",
    "print(keywords[:10])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1219d95d5844f6af4e0cf2878260c40dbf8742300f233955478158a2d2740a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
