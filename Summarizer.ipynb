{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code Block 1\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import operator\n",
    "\n",
    "from data_loader import DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize ,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from nltk import pos_tag\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import pke\n",
    "\n",
    "from  itertools import chain\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lijia\\Desktop\\MSAI\\Sem A\\AI6122\\Assignment1\\Hearts-of-Iron\\data_loader.py:14: FutureWarning: Starting with pandas version 2.0 all arguments of read_json except for the argument 'path_or_buf' will be keyword-only.\n",
      "  self.table = pd.read_json(data_path, 'records', lines = True);\n"
     ]
    }
   ],
   "source": [
    "#Code Block 2\n",
    "loader = DataLoader(DataLoader.data_path1)\n",
    "table = loader.load_table()\n",
    "# table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code Block 3\n",
    "#Data Preprocess\n",
    "#Get stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "nltk_stopwords.append('\\n')\n",
    "spacy_stopwords = list(STOP_WORDS)\n",
    "merged_sw = nltk_stopwords + list(set(spacy_stopwords) - set(nltk_stopwords))\n",
    "\n",
    "# print(merged_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code Block 4\n",
    "#Create the review text dictionary\n",
    "reviewText = {}\n",
    "for i in table.index:\n",
    "    if table['asin'][i] in reviewText:\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)\n",
    "    else:\n",
    "        reviewText[table['asin'][i]] = []\n",
    "        chunk = table['reviewText'][i]\n",
    "        sentences = nltk.sent_tokenize(chunk)\n",
    "        for s in sentences:\n",
    "            reviewText[table['asin'][i]].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 5\n",
    "def data_preprocess(original_review, stop_words):\n",
    "    # to lowercase\n",
    "    review = original_review.lower()\n",
    "    # remove punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', ' ', review).strip()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    review_words = review.split(' ')\n",
    "    review_words_tag = nltk.pos_tag(review_words)\n",
    "    processed_review = \"\"\n",
    "    # remove stop words&lemma\n",
    "    for word, tag in review_words_tag:\n",
    "        wntag = tag[0].lower()\n",
    "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "        if not wntag:\n",
    "            lemma_word = word\n",
    "        else:\n",
    "            lemma_word = lemmatizer.lemmatize(word, wntag)\n",
    "        if lemma_word not in stop_words:\n",
    "            processed_review = processed_review + lemma_word + \" \"\n",
    "    processed_review = \" \".join(processed_review.split())\n",
    "    processed_review = processed_review.strip()\n",
    "    return processed_review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 6\n",
    "processed_reviewText = {}\n",
    "for r in reviewText:\n",
    "    processed_reviewText[r] = []\n",
    "    for t in reviewText[r]:\n",
    "        p = data_preprocess(t, merged_sw)\n",
    "        processed_reviewText[r].append(p)\n",
    "# print(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 7\n",
    "# collect frequency\n",
    "\n",
    "def collect_frequency(processed_review):\n",
    "    word_frequency = defaultdict(lambda:0)\n",
    "    for v in processed_review.values():\n",
    "        for i in v:\n",
    "            tokens = nltk.word_tokenize(i)\n",
    "            for token in tokens:\n",
    "                word_frequency[token] += 1  \n",
    "    return word_frequency\n",
    "\n",
    "frequency = collect_frequency(processed_reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 8\n",
    "frequency = sorted(frequency.items(), key=lambda item: item[1],reverse=True)\n",
    "# print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 9\n",
    "sorted_processed_reviewText = sorted(processed_reviewText.items(), key=lambda item: len(item[1]),reverse=True)\n",
    "# print(sorted_processed_reviewText[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511\n"
     ]
    }
   ],
   "source": [
    "#Code Block 10\n",
    "print(len(processed_reviewText[sorted_processed_reviewText[1][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 11\n",
    "text = \"\"\"\"\"\"\n",
    "for i in reviewText[sorted_processed_reviewText[1][0]]:\n",
    "    text+=i\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 12\n",
    "# Baseline Models Cite from https://github.com/boudinfl/pke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sweet new adult love story', 0.01873074306463817), ('own little way', 0.01787713990583066), ('good new adult read', 0.01720642591694452), ('little more emotional', 0.016661953890430747), ('new adult book', 0.016143372941646507), ('little bit', 0.015503498780450842), ('my first erin mccarthy book', 0.015193289441674926), ('own little world', 0.014782913413971397), ('emotional new adult romance', 0.014368288213148221), ('little more', 0.014353641931933553)]\n",
      "2.2809557914733887\n"
     ]
    }
   ],
   "source": [
    "#Code Block 13\n",
    "start = time.time()\n",
    "#TextRank\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('robin', 0.00016692776126888526), ('phoenix', 0.0002285632064340584), ('true believers series', 0.00037332690716510616), ('erin mccarthy', 0.0007281100511081922), ('true believers', 0.000792142882191785), ('phoenix knows robin', 0.001223850272677977), ('new adult', 0.0012330087855895913), ('believe', 0.0016422877181582354), ('book', 0.0017387092834082662), ('true', 0.0018192728076754574)]\n",
      "4.630007743835449\n"
     ]
    }
   ],
   "source": [
    "#Code Block 14\n",
    "start = time.time()\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        normalization=None,\n",
    "                        stoplist=merged_sw)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8\n",
    "keyphrases = extractor.get_n_best(n=10, threshold=threshold)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('robin', 473.90378771562575), ('phoenix', 404.1654376838949), ('book', 282.1233251283658), ('series', 215.55490009807727), ('story', 199.70527509086568), ('love', 183.85565008365413), ('like', 164.83610007500025), ('characters', 163.25113757427908), ('loved', 136.30677506201945), ('believe', 134.7218125612983)]\n",
      "3.496669292449951\n"
     ]
    }
   ],
   "source": [
    "#Code Block 15\n",
    "start = time.time()\n",
    "# 1. create a TfIdf extractor.\n",
    "extractor = pke.unsupervised.TfIdf()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += pke.lang.stopwords.get('en')\n",
    "extractor.load_document(input=text,\n",
    "                        language='en',\n",
    "                        stoplist=merged_sw,\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks as candidates.\n",
    "extractor.candidate_selection(n=3)\n",
    "\n",
    "# 4. weight the candidates using a `tf` x `idf`\n",
    "df = pke.load_document_frequency_file(input_file='df.tsv.gz')\n",
    "extractor.candidate_weighting(df=df)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('robin', 0.04257842931879103), ('phoenix', 0.03774593671528739), ('book', 0.02269715602136298), ('story', 0.01653469865746098), ('true believer', 0.0162741744072179), ('characters', 0.015724353540855395), ('entire series', 0.014671122994158304), ('friend', 0.010583860377464727), ('love', 0.009111823065316186), ('sweet', 0.008079805440531326)]\n",
      "18.57978367805481\n"
     ]
    }
   ],
   "source": [
    "#Code Block 16\n",
    "start = time.time()\n",
    "# TopicRank\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load text\n",
    "extractor.load_document(input=text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "print(keyphrases)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 17\n",
    "# RAKE refers to https://github.com/fabianvf/python-rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 18\n",
    "# RAKE Algorithm\n",
    "def is_num(current_word):\n",
    "    try:\n",
    "        float(s) if '.' in s else int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "        \n",
    "def split_words(text):\n",
    "    splitter = re.compile(r'(?u)\\W+')\n",
    "    \n",
    "    words = []\n",
    "    for singleWord in splitter.split(text):\n",
    "        currentWord = singleWord.strip().lower()\n",
    "        \n",
    "        if currentWord != '' and not is_num(currentWord):\n",
    "            words.append(currentWord)\n",
    "            \n",
    "    return words\n",
    "\n",
    "\n",
    "def build_regex(stopwords):\n",
    "    sw_regex_list = []\n",
    "    for word in stopwords:\n",
    "        word_regex = r'\\b' + word + r'(?![\\w-])'\n",
    "        sw_regex_list.append(word_regex)\n",
    "        \n",
    "    return re.compile('(?u)' + '|'.join(sw_regex_list), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def generate_keywords(sentences, stopWordPattern, minCharacters, maxWords):\n",
    "    phrases = []\n",
    "    for s in sentences:\n",
    "        tmp = re.sub(stopWordPattern, '|', s.strip())\n",
    "        ps = tmp.split(\"|\")\n",
    "        \n",
    "        for phrase in ps:\n",
    "            phrase = phrase.strip().lower()\n",
    "            \n",
    "            if phrase != '' and len(phrase) >= minCharacters and len(phrase.split()) <= maxWords:\n",
    "                phrases.append(phrase)\n",
    "                \n",
    "    return phrases\n",
    "\n",
    "\n",
    "def word_scores(phraseList):\n",
    "    frequency = {}\n",
    "    degree = {}\n",
    "    for phrase in phraseList:\n",
    "        wordList = split_words(phrase)\n",
    "        wordListLen = len(wordList)\n",
    "        wordListDegree = wordListLen - 1\n",
    "        for word in wordList:\n",
    "            frequency.setdefault(word, 0)\n",
    "            frequency[word] += 1\n",
    "            degree.setdefault(word, 0)\n",
    "            degree[word] += wordListDegree\n",
    "    for i in frequency:\n",
    "        degree[i] = frequency[i] + degree[i]\n",
    "    score = {}\n",
    "    for i in frequency:\n",
    "        score.setdefault(i, 0)\n",
    "        score[i] = degree[i] / (frequency[i] * 1.0)\n",
    "    return score\n",
    "\n",
    "\n",
    "def generate_candidate_keyword_scores(phraseList, wordScore, minFrequency):\n",
    "    ckScore = {}\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    for p in phraseList:\n",
    "        counts[p] += 1\n",
    "\n",
    "    for phrase in phraseList:\n",
    "        if counts[phrase] >= minFrequency:\n",
    "            ckScore.setdefault(phrase, 0)\n",
    "            wordList = split_words(phrase)\n",
    "            candidateScore = 0\n",
    "            for word in wordList:\n",
    "                candidateScore += wordScore[word]\n",
    "            ckScore[phrase] = candidateScore\n",
    "    return ckScore\n",
    "\n",
    "def Rake(text, minCharNum, maxWordNum, minFrequency):\n",
    "        sentences = re.compile(u'[.!?,;:\\t\\\\\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013]|\\\\s\\\\-\\\\s').split(text)\n",
    "\n",
    "        phrases = generate_keywords(sentences, build_regex(merged_sw), minCharNum, maxWordNum)\n",
    "\n",
    "        wordScores = word_scores(phrases)\n",
    "\n",
    "        ck = generate_candidate_keyword_scores(phrases, wordScores, minFrequency)\n",
    "\n",
    "        sortedKeywords = sorted(ck.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        return sortedKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('typical college party girl', 9.99688013136289), ('new adult genre', 8.202836879432624), ('january 21st 2014', 8.166666666666666), ('penguin group usa', 7.875), ('intermittent explosive disorder', 7.65), ('new adult series', 7.630764807360553), ('new adult books', 7.518928833455613), ('new adult book', 7.413829787234043), ('nal / signet romance', 7.328205128205129), ('erin mccarthy brings', 7.29201680672269)]\n",
      "0.257007360458374\n"
     ]
    }
   ],
   "source": [
    "#Code Block 19\n",
    "start = time.time()\n",
    "\n",
    "keywords = Rake(text, 2, 4, 2)\n",
    "print(keywords[:10])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1219d95d5844f6af4e0cf2878260c40dbf8742300f233955478158a2d2740a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
